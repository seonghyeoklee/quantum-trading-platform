groups:
  # 핵심 시스템 알림
  - name: quantum-system-alerts
    rules:
      # 서비스 다운 알림
      - alert: ServiceDown
        expr: up{job=~"quantum-.*"} == 0
        for: 30s
        labels:
          severity: critical
          service: "{{ $labels.job }}"
        annotations:
          summary: "🚨 {{ $labels.job }} 서비스 중단"
          description: "{{ $labels.job }} 서비스가 30초 이상 응답하지 않습니다. 즉시 확인이 필요합니다."
      
      # 서비스 복구 알림
      - alert: ServiceRecovered
        expr: up{job=~"quantum-.*"} == 1
        for: 30s
        labels:
          severity: info
          service: "{{ $labels.job }}"
        annotations:
          summary: "✅ {{ $labels.job }} 서비스 복구"
          description: "{{ $labels.job }} 서비스가 정상적으로 복구되었습니다."

  # HTTP 성능 및 에러 알림
  - name: quantum-http-alerts
    rules:
      # 높은 에러율 (Critical)
      - alert: CriticalErrorRate
        expr: (rate(http_server_requests_seconds_count{status=~"5.."}[5m]) / rate(http_server_requests_seconds_count[5m])) > 0.25
        for: 2m
        labels:
          severity: critical
          service: "{{ $labels.job }}"
        annotations:
          summary: "🔥 {{ $labels.job }} 심각한 에러율"
          description: "{{ $labels.job }}에서 5분간 HTTP 5xx 에러율이 25%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # 높은 에러율 (Warning)
      - alert: HighErrorRate
        expr: (rate(http_server_requests_seconds_count{status=~"5.."}[5m]) / rate(http_server_requests_seconds_count[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.job }}"
        annotations:
          summary: "⚠️ {{ $labels.job }} 높은 에러율"
          description: "{{ $labels.job }}에서 5분간 HTTP 5xx 에러율이 10%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # 응답 시간 지연 (Critical)
      - alert: CriticalResponseTime
        expr: histogram_quantile(0.95, rate(http_server_requests_seconds_bucket[5m])) > 5.0
        for: 2m
        labels:
          severity: critical
          service: "{{ $labels.job }}"
        annotations:
          summary: "🔥 {{ $labels.job }} 심각한 응답 지연"
          description: "{{ $labels.job }}의 95th percentile 응답시간이 5초를 초과했습니다. (현재: {{ $value }}초)"
      
      # 응답 시간 지연 (Warning)
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_server_requests_seconds_bucket[5m])) > 2.0
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.job }}"
        annotations:
          summary: "⚠️ {{ $labels.job }} 응답 지연"
          description: "{{ $labels.job }}의 95th percentile 응답시간이 2초를 초과했습니다. (현재: {{ $value }}초)"

  # 시스템 리소스 알림
  - name: quantum-resource-alerts
    rules:
      # 메모리 사용률 높음 (Critical)
      - alert: CriticalMemoryUsage
        expr: (container_memory_usage_bytes{name=~"quantum-.*"} / container_spec_memory_limit_bytes{name=~"quantum-.*"}) > 0.95
        for: 2m
        labels:
          severity: critical
          container: "{{ $labels.name }}"
        annotations:
          summary: "🔥 {{ $labels.name }} 메모리 위험"
          description: "{{ $labels.name }} 컨테이너의 메모리 사용률이 95%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # 메모리 사용률 높음 (Warning)
      - alert: HighMemoryUsage
        expr: (container_memory_usage_bytes{name=~"quantum-.*"} / container_spec_memory_limit_bytes{name=~"quantum-.*"}) > 0.85
        for: 5m
        labels:
          severity: warning
          container: "{{ $labels.name }}"
        annotations:
          summary: "⚠️ {{ $labels.name }} 높은 메모리 사용"
          description: "{{ $labels.name }} 컨테이너의 메모리 사용률이 85%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # CPU 사용률 높음 (Warning)
      - alert: HighCpuUsage
        expr: rate(container_cpu_usage_seconds_total{name=~"quantum-.*"}[5m]) * 100 > 80
        for: 10m
        labels:
          severity: warning
          container: "{{ $labels.name }}"
        annotations:
          summary: "⚠️ {{ $labels.name }} 높은 CPU 사용률"
          description: "{{ $labels.name }} 컨테이너의 CPU 사용률이 80%를 초과했습니다. (현재: {{ $value }}%)"

  # 데이터베이스 알림
  - name: quantum-database-alerts
    rules:
      # PostgreSQL 연결 수 높음
      - alert: HighPostgreSQLConnections
        expr: pg_stat_database_numbackends{datname="quantum_trading"} > 50
        for: 5m
        labels:
          severity: warning
          database: "postgresql"
        annotations:
          summary: "⚠️ PostgreSQL 높은 연결 수"
          description: "PostgreSQL 데이터베이스의 활성 연결이 50개를 초과했습니다. (현재: {{ $value }}개)"
      
      # Redis 메모리 사용률 높음
      - alert: HighRedisMemoryUsage
        expr: (redis_memory_used_bytes / redis_config_maxmemory) > 0.9
        for: 5m
        labels:
          severity: warning
          database: "redis"
        annotations:
          summary: "⚠️ Redis 높은 메모리 사용률"
          description: "Redis의 메모리 사용률이 90%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # Redis 캐시 히트율 낮음
      - alert: LowRedisCacheHitRate
        expr: (redis_keyspace_hits_total / (redis_keyspace_hits_total + redis_keyspace_misses_total)) < 0.8
        for: 10m
        labels:
          severity: warning
          database: "redis"
        annotations:
          summary: "⚠️ Redis 낮은 캐시 히트율"
          description: "Redis 캐시 히트율이 80% 미만입니다. (현재: {{ $value | humanizePercentage }})"

  # Kafka 메시지 브로커 알림
  - name: quantum-kafka-alerts
    rules:
      # Consumer Lag 높음 (Critical)
      - alert: CriticalKafkaConsumerLag
        expr: kafka_consumer_lag_sum > 1000
        for: 2m
        labels:
          severity: critical
          service: "kafka"
          topic: "{{ $labels.topic }}"
        annotations:
          summary: "🔥 Kafka Consumer Lag 심각"
          description: "{{ $labels.topic }} 토픽의 Consumer Lag이 1000을 초과했습니다. (현재: {{ $value }})"
      
      # Consumer Lag 높음 (Warning)
      - alert: HighKafkaConsumerLag
        expr: kafka_consumer_lag_sum > 500
        for: 5m
        labels:
          severity: warning
          service: "kafka"
          topic: "{{ $labels.topic }}"
        annotations:
          summary: "⚠️ Kafka Consumer Lag 높음"
          description: "{{ $labels.topic }} 토픽의 Consumer Lag이 500을 초과했습니다. (현재: {{ $value }})"

  # 비즈니스 메트릭 알림
  - name: quantum-business-alerts
    rules:
      # 거래 처리 실패율 높음
      - alert: HighTradingFailureRate
        expr: (rate(quantum_trading_orders_total{status="failed"}[5m]) / rate(quantum_trading_orders_total[5m])) > 0.05
        for: 3m
        labels:
          severity: warning
          type: "business"
        annotations:
          summary: "⚠️ 높은 거래 실패율"
          description: "거래 주문 실패율이 5%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # 시장 데이터 지연
      - alert: HighMarketDataLatency
        expr: histogram_quantile(0.95, rate(quantum_market_data_latency_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
          type: "business"
        annotations:
          summary: "⚠️ 시장 데이터 지연"
          description: "시장 데이터 처리 지연시간이 500ms를 초과했습니다. (현재: {{ $value | humanizeDuration }})"
      
      # 거래량 급감 감지
      - alert: LowTradingVolume
        expr: rate(quantum_trading_orders_total[10m]) < 0.1
        for: 15m
        labels:
          severity: info
          type: "business"
        annotations:
          summary: "ℹ️ 낮은 거래량"
          description: "10분간 평균 거래량이 분당 0.1건 미만입니다. 시스템 점검이 필요할 수 있습니다."

  # 로그 기반 알림
  - name: quantum-log-alerts
    rules:
      # 높은 로그 에러율 (Critical)
      - alert: CriticalLogErrorRate
        expr: (rate(log_errors_total[5m]) / rate(log_entries_total[5m])) > 0.1
        for: 2m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "🔥 {{ $labels.service }} 심각한 로그 에러율"
          description: "{{ $labels.service }}에서 5분간 로그 에러율이 10%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # 로그 에러 급증 (Warning)
      - alert: HighLogErrorCount
        expr: rate(log_errors_total[5m]) > 0.5
        for: 3m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "⚠️ {{ $labels.service }} 로그 에러 급증"
          description: "{{ $labels.service}}에서 분당 0.5개 이상의 에러 로그가 발생하고 있습니다. (현재: {{ $value }}/분)"
      
      # HTTP 4xx 에러 급증
      - alert: HighHttp4xxErrorLogs
        expr: rate(http_requests_log_total{http_status=~"4.."}[5m]) > 2.0
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "⚠️ {{ $labels.service }} HTTP 4xx 에러 급증"
          description: "{{ $labels.service }}에서 분당 2건 이상의 4xx HTTP 에러가 로그에 기록되고 있습니다. (현재: {{ $value }}/분)"
      
      # HTTP 5xx 에러 발생 (Critical)
      - alert: CriticalHttp5xxErrorLogs
        expr: rate(http_requests_log_total{http_status=~"5.."}[5m]) > 0.1
        for: 1m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "🔥 {{ $labels.service }} HTTP 5xx 서버 에러"
          description: "{{ $labels.service }}에서 5xx 서버 에러가 로그에 기록되고 있습니다. (현재: {{ $value }}/분)"
      
      # Exception/Error 패턴 감지
      - alert: ApplicationExceptionDetected
        expr: rate(log_errors_total{error_type=~"Exception|Error"}[2m]) > 0
        for: 30s
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "⚠️ {{ $labels.service }} 애플리케이션 예외 발생"
          description: "{{ $labels.service }}에서 {{ $labels.error_type }}이(가) 로그에서 감지되었습니다."
      
      # 로그 수집 중단 감지
      - alert: LogCollectionDown
        expr: absent_over_time(log_entries_total[5m])
        for: 2m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "🚨 {{ $labels.service }} 로그 수집 중단"
          description: "{{ $labels.service }}에서 5분 이상 로그가 수집되지 않고 있습니다. Promtail 또는 로그 파이프라인 점검이 필요합니다."

  # 고급 로그 분석 알림
  - name: quantum-advanced-log-alerts
    rules:
      # SQL 에러 급증 감지
      - alert: SQLErrorSpike
        expr: rate(sql_errors_total[5m]) > 0.2
        for: 2m
        labels:
          severity: critical
          service: "{{ $labels.service }}"
          type: "logs"
          database: "sql"
        annotations:
          summary: "🗄️ {{ $labels.service }} SQL 에러 급증"
          description: "{{ $labels.service }}에서 분당 0.2건 이상의 SQL 에러가 발생하고 있습니다. 데이터베이스 연결 또는 쿼리를 점검해주세요."
      
      # 네트워크 에러 급증 감지
      - alert: NetworkErrorSpike
        expr: rate(network_errors_total[5m]) > 0.3
        for: 3m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "logs"
          category: "network"
        annotations:
          summary: "🌐 {{ $labels.service }} 네트워크 에러 급증"
          description: "{{ $labels.service }}에서 분당 0.3건 이상의 네트워크 에러가 발생하고 있습니다. 외부 서비스 연결 상태를 확인해주세요."
      
      # 인증/권한 에러 패턴 감지
      - alert: AuthenticationFailureSpike
        expr: rate(auth_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "logs"
          category: "security"
        annotations:
          summary: "🔐 {{ $labels.service }} 인증 실패 급증"
          description: "{{ $labels.service }}에서 분당 0.1건 이상의 인증/권한 에러가 발생하고 있습니다. 보안 사고 가능성을 점검해주세요."
      
      # 비즈니스 로직 에러 급증
      - alert: BusinessLogicErrorSpike
        expr: rate(business_errors_total[5m]) > 0.15
        for: 3m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "business"
        annotations:
          summary: "💼 {{ $labels.service }} 비즈니스 로직 에러 급증"
          description: "{{ $labels.service }}에서 분당 0.15건 이상의 비즈니스 로직 에러가 발생하고 있습니다. 거래 처리 로직을 점검해주세요."
      
      # 특정 API 엔드포인트 에러 급증
      - alert: APIEndpointErrorSpike
        expr: |
          (
            sum by (api_endpoint, service) (rate(business_errors_total[5m])) +
            sum by (api_endpoint, service) (rate(sql_errors_total[5m])) +
            sum by (api_endpoint, service) (rate(network_errors_total[5m]))
          ) /
          sum by (api_endpoint, service) (rate(api_requests_log_total[5m])) > 0.05
        for: 2m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          api_endpoint: "{{ $labels.api_endpoint }}"
          type: "logs"
        annotations:
          summary: "🎯 API {{ $labels.api_endpoint }} 에러율 급증"
          description: "API /{{ $labels.api_endpoint }}에서 에러율이 5%를 초과했습니다. (현재: {{ $value | humanizePercentage }})"
      
      # 에러 카테고리 불균형 감지 (한 카테고리가 전체의 80% 이상)
      - alert: ErrorCategoryImbalance
        expr: |
          (
            max by (service) (rate(sql_errors_total[5m])) or
            max by (service) (rate(network_errors_total[5m])) or
            max by (service) (rate(auth_errors_total[5m])) or
            max by (service) (rate(business_errors_total[5m]))
          ) / 
          sum by (service) (rate(error_categories_total[5m])) > 0.8
        for: 5m
        labels:
          severity: info
          service: "{{ $labels.service }}"
          type: "logs"
        annotations:
          summary: "📊 {{ $labels.service }} 에러 패턴 편중"
          description: "{{ $labels.service }}에서 특정 에러 카테고리가 전체 에러의 80% 이상을 차지하고 있습니다. 근본 원인 분석이 필요합니다."
      
      # 사용자 활동 급감 감지
      - alert: UserActivityDropoff
        expr: |
          (
            sum(rate(user_actions_total[5m])) < 
            sum(rate(user_actions_total[1h] offset 1h)) * 0.3
          )
        for: 10m
        labels:
          severity: warning
          type: "business"
        annotations:
          summary: "👥 사용자 활동 급감"
          description: "현재 사용자 활동이 1시간 전 대비 70% 이상 감소했습니다. 시스템 장애 또는 서비스 이슈를 점검해주세요."
      
      # API 응답시간 로그 기반 지연 감지
      - alert: APIResponseTimeFromLogs
        expr: histogram_quantile(0.95, rate(api_response_time_log_bucket[5m])) > 2000
        for: 3m
        labels:
          severity: warning
          service: "{{ $labels.service }}"
          type: "performance"
        annotations:
          summary: "⏱️ {{ $labels.service }} 로그 기반 응답시간 지연"
          description: "로그에서 추출한 P95 응답시간이 2초를 초과했습니다. (현재: {{ $value }}ms)"
      
      # 거래 ID 기반 문제 거래 감지
      - alert: ProblematicTradingPattern
        expr: |
          sum by (trade_id) (rate(business_errors_total[5m])) > 0
          and
          sum by (trade_id) (rate(trading_events_total[5m])) > 0.1
        for: 1m
        labels:
          severity: critical
          trade_id: "{{ $labels.trade_id }}"
          type: "business"
        annotations:
          summary: "💰 문제 거래 패턴 감지"
          description: "거래 ID {{ $labels.trade_id }}에서 비즈니스 에러가 발생하면서 동시에 높은 활동을 보이고 있습니다. 즉시 점검이 필요합니다."

  # 간단한 테스트 알림 (수동 트리거용)
  - name: quantum-test-alerts
    rules:
      - alert: TestAlert
        expr: prometheus_build_info > 0
        for: 0s
        labels:
          severity: info
          type: "test"
        annotations:
          summary: "🧪 Quantum Trading Platform - 테스트 알림"
          description: "이것은 알림 시스템 테스트용 알림입니다. Prometheus가 정상 작동 중입니다."