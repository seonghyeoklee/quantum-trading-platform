"""
Kafka ì´ë²¤íŠ¸ ë°œí–‰ì

í‚¤ì›€ API ë°ì´í„°ë¥¼ Kafkaë¡œ ë¹„ë™ê¸° ë°œí–‰í•˜ëŠ” Publisher
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, Union, List
from contextlib import asynccontextmanager

from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError

from .event_schemas import BaseEvent, EventType


logger = logging.getLogger(__name__)


class KafkaEventPublisher:
    """ë¹„ë™ê¸° Kafka ì´ë²¤íŠ¸ ë°œí–‰ì"""
    
    def __init__(
        self, 
        bootstrap_servers: str = "quantum-kafka:9092",
        enable_kafka: bool = True,
        batch_size: int = 100,
        max_batch_size: int = 16384
    ):
        self.bootstrap_servers = bootstrap_servers
        self.enable_kafka = enable_kafka
        self.batch_size = batch_size
        self.max_batch_size = max_batch_size
        
        self.producer: Optional[AIOKafkaProducer] = None
        self.is_connected = False
        self._batch_buffer: List[tuple] = []
        self._last_flush = datetime.now()
        
        # í† í”½ ì„¤ì • (ê¸°ì¡´ ë“±ë¡ëœ í† í”½ì— ë§ì¶¤)
        self.topics = {
            EventType.STOCK_TRADE: "kiwoom.realtime.quote",      # ì‹¤ì‹œê°„ ì‹œì„¸/ì²´ê²° â†’ quote
            EventType.STOCK_PRICE: "kiwoom.realtime.quote",      # ì‹¤ì‹œê°„ í˜„ì¬ê°€ â†’ quote
            EventType.ORDER_BOOK: "kiwoom.realtime.quote",       # ì‹¤ì‹œê°„ í˜¸ê°€ â†’ quote
            EventType.ORDER_EXECUTED: "kiwoom.realtime.order",   # ê±°ë˜ ì£¼ë¬¸ â†’ order
            EventType.ORDER_FAILED: "kiwoom.realtime.order",     # ì£¼ë¬¸ ì‹¤íŒ¨ â†’ order
            EventType.MARKET_DATA: "kiwoom.realtime.general",    # ì‹œì¥ ë°ì´í„° â†’ general
            EventType.SCREENING_SIGNAL: "kiwoom.realtime.screener", # ì¡°ê±´ê²€ìƒ‰ â†’ screener
            EventType.API_RESPONSE: "kiwoom.realtime.general",   # ì¼ë°˜ API â†’ general
            EventType.API_ERROR: "kiwoom.realtime.general"       # ì—ëŸ¬ ì´ë²¤íŠ¸ â†’ general
        }
        
        logger.info(f"KafkaEventPublisher ì´ˆê¸°í™” - í™œì„±í™”: {enable_kafka}")
        
    async def start(self):
        """Kafka Producer ì‹œì‘"""
        if not self.enable_kafka:
            logger.info("Kafka ë¹„í™œì„±í™” ìƒíƒœ - ì´ë²¤íŠ¸ëŠ” ë¡œê·¸ë¡œë§Œ ì¶œë ¥ë©ë‹ˆë‹¤")
            return
            
        try:
            self.producer = AIOKafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=self._serialize_event,
                key_serializer=lambda k: k.encode('utf-8') if k else None,
                acks='all',  # ëª¨ë“  ë ˆí”Œë¦¬ì¹´ì—ì„œ í™•ì¸
                compression_type='gzip',
                request_timeout_ms=30000,
                enable_idempotence=True  # ì¤‘ë³µ ë°©ì§€
            )
            
            await self.producer.start()
            self.is_connected = True
            logger.info(f"âœ… Kafka Producer ì—°ê²° ì„±ê³µ: {self.bootstrap_servers}")
            
            # ë°°ì¹˜ í”ŒëŸ¬ì‹œ íƒœìŠ¤í¬ ì‹œì‘
            asyncio.create_task(self._batch_flush_worker())
            
        except Exception as e:
            logger.error(f"âŒ Kafka Producer ì—°ê²° ì‹¤íŒ¨: {e}")
            self.is_connected = False
            
    async def stop(self):
        """Kafka Producer ì¢…ë£Œ"""
        if self.producer and self.is_connected:
            try:
                # ë‚¨ì€ ë°°ì¹˜ í”ŒëŸ¬ì‹œ
                await self._flush_batch()
                await self.producer.stop()
                self.is_connected = False
                logger.info("Kafka Producer ì—°ê²° ì¢…ë£Œ")
            except Exception as e:
                logger.error(f"Kafka Producer ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
                
    def _serialize_event(self, event_data: Union[Dict, BaseEvent]) -> bytes:
        """ì´ë²¤íŠ¸ ì§ë ¬í™”"""
        if isinstance(event_data, BaseEvent):
            data = event_data.model_dump(mode='json')
        else:
            data = event_data
            
        return json.dumps(data, ensure_ascii=False, default=str).encode('utf-8')
        
    def _get_topic_name(self, event_type: Union[EventType, str]) -> str:
        """ì´ë²¤íŠ¸ íƒ€ì…ì— ë”°ë¥¸ í† í”½ëª… ê²°ì •"""
        if isinstance(event_type, str):
            try:
                event_type = EventType(event_type)
            except ValueError:
                return "kiwoom.misc.events"  # ê¸°ë³¸ í† í”½
                
        return self.topics.get(event_type, "kiwoom.misc.events")
        
    async def publish_event(
        self, 
        event: Union[BaseEvent, Dict[str, Any]],
        topic: Optional[str] = None,
        key: Optional[str] = None,
        headers: Optional[Dict[str, str]] = None
    ) -> bool:
        """ë‹¨ì¼ ì´ë²¤íŠ¸ ë°œí–‰"""
        try:
            # ì´ë²¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            if isinstance(event, BaseEvent):
                event_type = event.event_type
                event_data = event
                if not key:
                    key = getattr(event, 'symbol', None) or event.event_id
            else:
                event_type = event.get('event_type', EventType.API_RESPONSE)
                event_data = event
                if not key:
                    key = event.get('symbol') or str(uuid.uuid4())
            
            # í† í”½ ê²°ì •
            if not topic:
                topic = self._get_topic_name(event_type)
                
            # Kafka ë¹„í™œì„±í™” ì‹œ ë¡œê·¸ë§Œ ì¶œë ¥
            if not self.enable_kafka:
                logger.info(f"ğŸ“¤ [MOCK] Eventë°œí–‰: {topic} - {event_type}")
                return True
                
            if not self.is_connected:
                logger.warning("Kafka ì—°ê²° ëŠê¹€ - ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨")
                return False
                
            # Kafkaë¡œ ì „ì†¡
            await self.producer.send_and_wait(
                topic=topic,
                value=event_data,
                key=key,
                headers=self._prepare_headers(headers)
            )
            
            logger.debug(f"ğŸ“¤ ì´ë²¤íŠ¸ ë°œí–‰ ì„±ê³µ: {topic} - {event_type}")
            return True
            
        except KafkaError as e:
            logger.error(f"âŒ Kafka ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ ì´ë²¤íŠ¸ ë°œí–‰ ì¤‘ ì˜ˆì™¸: {e}")
            return False
            
    async def publish_batch(
        self, 
        events: List[Union[BaseEvent, Dict[str, Any]]],
        topic: Optional[str] = None
    ) -> int:
        """ë°°ì¹˜ ì´ë²¤íŠ¸ ë°œí–‰"""
        if not events:
            return 0
            
        success_count = 0
        
        if not self.enable_kafka:
            logger.info(f"ğŸ“¤ [MOCK] ë°°ì¹˜ Eventë°œí–‰: {len(events)}ê°œ ì´ë²¤íŠ¸")
            return len(events)
            
        if not self.is_connected:
            logger.warning("Kafka ì—°ê²° ëŠê¹€ - ë°°ì¹˜ ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨")
            return 0
            
        try:
            # ë°°ì¹˜ë¡œ ì „ì†¡
            for event in events:
                if isinstance(event, BaseEvent):
                    event_topic = topic or self._get_topic_name(event.event_type)
                    key = getattr(event, 'symbol', None) or event.event_id
                else:
                    event_topic = topic or self._get_topic_name(event.get('event_type'))
                    key = event.get('symbol') or str(uuid.uuid4())
                    
                await self.producer.send(
                    topic=event_topic,
                    value=event,
                    key=key
                )
                success_count += 1
                
            # ë°°ì¹˜ ì „ì†¡
            await self.producer.flush()
            logger.info(f"ğŸ“¤ ë°°ì¹˜ ì´ë²¤íŠ¸ ë°œí–‰ ì„±ê³µ: {success_count}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨: {e}")
            
        return success_count
        
    def _prepare_headers(self, headers: Optional[Dict[str, str]]) -> Optional[List[tuple]]:
        """í—¤ë” ì¤€ë¹„"""
        if not headers:
            headers = {}
            
        headers.update({
            'source': 'kiwoom_adapter',
            'timestamp': datetime.now().isoformat(),
            'content-type': 'application/json'
        })
        
        return [(k, v.encode('utf-8')) for k, v in headers.items()]
        
    async def _batch_flush_worker(self):
        """ë°°ì¹˜ í”ŒëŸ¬ì‹œ ì›Œì»¤ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)"""
        while self.is_connected:
            try:
                await asyncio.sleep(1.0)  # 1ì´ˆë§ˆë‹¤ ì²´í¬
                
                if (datetime.now() - self._last_flush).seconds >= 5:  # 5ì´ˆë§ˆë‹¤ í”ŒëŸ¬ì‹œ
                    await self._flush_batch()
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ë°°ì¹˜ í”ŒëŸ¬ì‹œ ì›Œì»¤ ì˜¤ë¥˜: {e}")
                
    async def _flush_batch(self):
        """ë°°ì¹˜ ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if not self._batch_buffer or not self.is_connected:
            return
            
        try:
            # ë²„í¼ì— ìˆëŠ” ì´ë²¤íŠ¸ë“¤ ì „ì†¡
            for topic, key, value in self._batch_buffer:
                await self.producer.send(topic=topic, key=key, value=value)
                
            await self.producer.flush()
            batch_size = len(self._batch_buffer)
            self._batch_buffer.clear()
            self._last_flush = datetime.now()
            
            logger.debug(f"ë°°ì¹˜ í”ŒëŸ¬ì‹œ ì™„ë£Œ: {batch_size}ê°œ ì´ë²¤íŠ¸")
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ í”ŒëŸ¬ì‹œ ì‹¤íŒ¨: {e}")
            
    async def get_connection_info(self) -> Dict[str, Any]:
        """ì—°ê²° ì •ë³´ ë°˜í™˜"""
        return {
            'bootstrap_servers': self.bootstrap_servers,
            'is_connected': self.is_connected,
            'enable_kafka': self.enable_kafka,
            'topics': self.topics,
            'batch_buffer_size': len(self._batch_buffer)
        }


# ì „ì—­ Publisher ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_publisher: Optional[KafkaEventPublisher] = None


async def get_kafka_publisher() -> KafkaEventPublisher:
    """ì „ì—­ Kafka Publisher ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_publisher
    
    if _global_publisher is None:
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
        import os
        
        enable_kafka = os.getenv('ENABLE_KAFKA', 'true').lower() == 'true'
        bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'quantum-kafka:9092')
        
        _global_publisher = KafkaEventPublisher(
            bootstrap_servers=bootstrap_servers,
            enable_kafka=enable_kafka
        )
        
        if enable_kafka:
            await _global_publisher.start()
            
    return _global_publisher


@asynccontextmanager
async def kafka_publisher_context():
    """Kafka Publisher ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    publisher = await get_kafka_publisher()
    try:
        yield publisher
    finally:
        if publisher and publisher.enable_kafka:
            await publisher.stop()