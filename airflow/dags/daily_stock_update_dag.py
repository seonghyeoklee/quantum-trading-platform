"""
ê¸°ì¡´ ì¢…ëª© ì¼ì¼ ì—…ë°ì´íŠ¸ DAG (Stock_Data__16_Daily_Update)
- domestic_stocks_detailì— ì´ë¯¸ ìˆëŠ” ì¢…ëª©ë“¤ì˜ ìµœì‹  1ì¼ì¹˜ ë°ì´í„° ì¶”ê°€
- ì „ì²´ 3,902ê°œ ì¢…ëª© ëŒ€ìƒ
- ë§¤ì¼ ì¥ ë§ˆê° í›„ ì‹¤í–‰
- ì‹¤ì œ KIS API ì—°ë™
"""

import os
import json
import time
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import pytz

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook


# ========================================
# DAG ì„¤ì •
# ========================================
default_args = {
    'owner': 'quantum-trading',
    'depends_on_past': False,
    'start_date': datetime(2025, 9, 8, tzinfo=pytz.timezone("Asia/Seoul")),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=10),
    'execution_timeout': timedelta(hours=4),
}

dag = DAG(
    dag_id='Stock_Data__16_Daily_Update',
    default_args=default_args,
    description='ê¸°ì¡´ ì¢…ëª© ì¼ì¼ ì—…ë°ì´íŠ¸ - ì „ì²´ ì¢…ëª©ì˜ ìµœì‹  1ì¼ì¹˜ ë°ì´í„° ì¶”ê°€',
    schedule_interval='0 21 * * 1-5',  # ì£¼ì¤‘ ì˜¤í›„ 9ì‹œ (ì¥ ë§ˆê° í›„)
    max_active_runs=1,
    catchup=False,
    tags=['daily-update', 'all-stocks', 'incremental', 'kis-api', 'production'],
)

# ========================================
# ìƒìˆ˜ ì •ì˜
# ========================================

# ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
BATCH_SIZE = 30  # 30ê°œì”© ì²˜ë¦¬ (ì•ˆì •ì„± ì¤‘ì‹œ)
BATCH_DELAY = 20  # ë°°ì¹˜ ê°„ 20ì´ˆ ëŒ€ê¸°
REQUEST_DELAY = 0.05  # 50ms API í˜¸ì¶œ ê°„ê²© (ì´ˆë‹¹ 20íšŒ)

# ìš°ì„ ìˆœìœ„ ì¢…ëª© (ë¹ ë¥¸ ì²˜ë¦¬)
PRIORITY_STOCKS = [
    '005930',  # ì‚¼ì„±ì „ì
    '000660',  # SKí•˜ì´ë‹‰ìŠ¤
    '035420',  # NAVER
    '005380',  # í˜„ëŒ€ì°¨
    '000270',  # ê¸°ì•„
    '068270',  # ì…€íŠ¸ë¦¬ì˜¨
    '035720',  # ì¹´ì¹´ì˜¤
    '207940',  # ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤
    '006400',  # ì‚¼ì„±SDI
    '051910',  # LGí™”í•™
]

# ========================================
# KIS API í´ë¼ì´ì–¸íŠ¸ (ì¼ì¼ ì—…ë°ì´íŠ¸ìš©)
# ========================================

class KISDailyUpdateClient:
    """ì¼ì¼ ì—…ë°ì´íŠ¸ìš© KIS API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.adapter_base_url = "http://host.docker.internal:8000"
        self.request_delay = REQUEST_DELAY
        self.success_count = 0
        self.error_count = 0
    
    def get_latest_data(self, stock_code: str, retries: int = 2) -> Optional[Dict]:
        """ìµœì‹  1ì¼ì¹˜ ë°ì´í„° ì¡°íšŒ (ì¬ì‹œë„ í¬í•¨)"""
        for attempt in range(retries):
            try:
                url = f"{self.adapter_base_url}/domestic/chart/{stock_code}"
                params = {
                    'period': 'D',
                    'adj_price': 1
                }
                
                response = requests.get(url, params=params, timeout=20)
                response.raise_for_status()
                
                kis_data = response.json()
                
                if kis_data.get('rt_cd') != '0':
                    if attempt == 0:  # ì²« ë²ˆì§¸ ì‹œë„ì—ì„œë§Œ ë¡œê·¸
                        print(f"âš ï¸ {stock_code}: KIS API ì˜¤ë¥˜ - {kis_data.get('msg1', 'Unknown')}")
                    return None
                
                if not kis_data.get('output2'):
                    return None
                
                # ìµœì‹  ê±°ë˜ì¼ 1ì¼ì¹˜ë§Œ ë°˜í™˜ (output2ì˜ ì²« ë²ˆì§¸ ìš”ì†Œ)
                latest_data = kis_data['output2'][0] if kis_data['output2'] else None
                if latest_data:
                    self.success_count += 1
                    time.sleep(self.request_delay)
                    return {
                        'rt_cd': kis_data['rt_cd'],
                        'latest_data': latest_data,
                        'full_response': kis_data
                    }
                
                return None
                
            except Exception as e:
                if attempt == retries - 1:  # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë§Œ ë¡œê·¸
                    self.error_count += 1
                    print(f"âŒ {stock_code}: API í˜¸ì¶œ ì‹¤íŒ¨ - {e}")
                else:
                    time.sleep(1)  # ì¬ì‹œë„ ì „ 1ì´ˆ ëŒ€ê¸°
        
        return None

# ========================================
# í•µì‹¬ í•¨ìˆ˜ë“¤
# ========================================

def get_existing_stocks_status(**context):
    """ê¸°ì¡´ ì¢…ëª©ë“¤ì˜ ìµœì‹  ë°ì´í„° ìƒíƒœ í™•ì¸"""
    
    print("ğŸ” ê¸°ì¡´ ì¢…ëª© ë°ì´í„° ìƒíƒœ ë¶„ì„...")
    
    pg_hook = PostgresHook(postgres_conn_id='quantum_postgres')
    
    # ë°ì´í„°ê°€ ìˆëŠ” ì¢…ëª©ë“¤ê³¼ ìµœì‹  ê±°ë˜ì¼ ì¡°íšŒ
    status_sql = """
    SELECT 
        ds.stock_code,
        ds.stock_name,
        ds.market_type,
        COALESCE(MAX(dsd.trade_date), '1900-01-01'::date) as latest_date,
        COUNT(dsd.stock_code) as data_count
    FROM domestic_stocks ds
    LEFT JOIN domestic_stocks_detail dsd ON ds.stock_code = dsd.stock_code
    WHERE ds.is_active = true
    GROUP BY ds.stock_code, ds.stock_name, ds.market_type
    HAVING COUNT(dsd.stock_code) > 0  -- ë°ì´í„°ê°€ ìˆëŠ” ì¢…ëª©ë§Œ
    ORDER BY 
        CASE WHEN ds.stock_code = ANY(ARRAY{priority_list}) THEN 1 ELSE 2 END,
        ds.market_type, 
        ds.stock_code;
    """.format(priority_list=str(PRIORITY_STOCKS).replace("'", "''"))
    
    existing_stocks = pg_hook.get_records(status_sql)
    
    if not existing_stocks:
        print("âŒ ë°ì´í„°ê°€ ìˆëŠ” ê¸°ì¡´ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {'existing_count': 0, 'stocks': []}
    
    # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
    priority_stocks = []
    regular_stocks = []
    
    for row in existing_stocks:
        stock_data = {
            'stock_code': row[0],
            'stock_name': row[1],
            'market_type': row[2],
            'latest_date': row[3].strftime('%Y-%m-%d') if row[3] else '1900-01-01',
            'data_count': row[4]
        }
        
        if row[0] in PRIORITY_STOCKS:
            priority_stocks.append(stock_data)
        else:
            regular_stocks.append(stock_data)
    
    all_stocks = priority_stocks + regular_stocks
    
    print(f"ğŸ“Š ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì¢…ëª©: {len(all_stocks):,}ê°œ")
    print(f"  - ìš°ì„ ìˆœìœ„ ì¢…ëª©: {len(priority_stocks)}ê°œ")
    print(f"  - ì¼ë°˜ ì¢…ëª©: {len(regular_stocks):,}ê°œ")
    
    # ë°ì´í„° ë¶„í¬ í™•ì¸
    today = datetime.now(pytz.timezone("Asia/Seoul")).date()
    recent_count = sum(1 for stock in all_stocks 
                      if (today - datetime.strptime(stock['latest_date'], '%Y-%m-%d').date()).days <= 3)
    
    print(f"  - ìµœê·¼ 3ì¼ ë‚´ ë°ì´í„°: {recent_count:,}ê°œ ({recent_count/len(all_stocks)*100:.1f}%)")
    
    # XComìœ¼ë¡œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
    context['task_instance'].xcom_push(key='existing_stocks', value=all_stocks)
    
    return {
        'existing_count': len(all_stocks),
        'priority_count': len(priority_stocks),
        'regular_count': len(regular_stocks),
        'recent_data_count': recent_count,
        'status_time': datetime.now(pytz.timezone("Asia/Seoul")).isoformat()
    }


def update_priority_stocks(**context):
    """ìš°ì„ ìˆœìœ„ ì¢…ëª© ë¨¼ì € ì—…ë°ì´íŠ¸"""
    
    print("ğŸš€ ìš°ì„ ìˆœìœ„ ì¢…ëª© ì¼ì¼ ì—…ë°ì´íŠ¸ ì‹œì‘...")
    
    existing_stocks = context['task_instance'].xcom_pull(key='existing_stocks', task_ids='get_existing_stocks_status')
    priority_stocks = [stock for stock in existing_stocks if stock['stock_code'] in PRIORITY_STOCKS]
    
    if not priority_stocks:
        print("âœ… ìš°ì„ ìˆœìœ„ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {'updated_count': 0, 'total_records': 0}
    
    return process_stock_batch(priority_stocks, "ìš°ì„ ìˆœìœ„")


def update_regular_stocks(**context):
    """ì¼ë°˜ ì¢…ëª© ë°°ì¹˜ ì—…ë°ì´íŠ¸"""
    
    print("ğŸ“¦ ì¼ë°˜ ì¢…ëª© ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹œì‘...")
    
    existing_stocks = context['task_instance'].xcom_pull(key='existing_stocks', task_ids='get_existing_stocks_status')
    regular_stocks = [stock for stock in existing_stocks if stock['stock_code'] not in PRIORITY_STOCKS]
    
    if not regular_stocks:
        print("âœ… ì¼ë°˜ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {'updated_count': 0, 'total_records': 0}
    
    print(f"ğŸ“ˆ {len(regular_stocks):,}ê°œ ì¼ë°˜ ì¢…ëª©ì„ {BATCH_SIZE}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬...")
    
    return process_stock_batch(regular_stocks, "ì¼ë°˜")


def process_stock_batch(stocks: List[Dict], batch_type: str) -> Dict:
    """ì¢…ëª© ë°°ì¹˜ ì²˜ë¦¬ (ê³µí†µ í•¨ìˆ˜)"""
    
    kis_client = KISDailyUpdateClient()
    pg_hook = PostgresHook(postgres_conn_id='quantum_postgres')
    
    total_updated = 0
    total_records = 0
    total_errors = 0
    
    # ë°°ì¹˜ë³„ ì²˜ë¦¬
    for i in range(0, len(stocks), BATCH_SIZE):
        batch = stocks[i:i+BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        total_batches = (len(stocks) - 1) // BATCH_SIZE + 1
        
        print(f"ğŸ“¦ {batch_type} ë°°ì¹˜ {batch_num}/{total_batches}: {len(batch)}ê°œ ì¢…ëª© ì²˜ë¦¬ ì¤‘...")
        
        batch_updated = 0
        batch_records = 0
        
        for stock in batch:
            stock_code = stock['stock_code']
            
            try:
                # ìµœì‹  ë°ì´í„° ì¡°íšŒ
                result = kis_client.get_latest_data(stock_code)
                
                if result and result['latest_data']:
                    latest_data = result['latest_data']
                    
                    # ì¤‘ë³µ í™•ì¸ (ì´ë¯¸ ìˆëŠ” ë‚ ì§œë©´ ìŠ¤í‚µ)
                    trade_date = latest_data['stck_bsop_date']
                    if not should_update_data(pg_hook, stock_code, trade_date):
                        continue
                    
                    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                    saved = save_daily_update_to_db(pg_hook, stock_code, latest_data, result['full_response'])
                    if saved:
                        batch_updated += 1
                        batch_records += 1
                        
                        if batch_updated <= 3:  # ì²˜ìŒ 3ê°œë§Œ ìƒì„¸ ë¡œê·¸
                            print(f"âœ… {stock_code}: {trade_date} ë°ì´í„° ì—…ë°ì´íŠ¸")
                
            except Exception as e:
                total_errors += 1
                print(f"âŒ {stock_code} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                continue
        
        total_updated += batch_updated
        total_records += batch_records
        
        print(f"  ë°°ì¹˜ {batch_num} ì™„ë£Œ: {batch_updated}/{len(batch)}ê°œ ì—…ë°ì´íŠ¸")
        
        # ë°°ì¹˜ ê°„ íœ´ì‹ (ë§ˆì§€ë§‰ ë°°ì¹˜ ì œì™¸)
        if i + BATCH_SIZE < len(stocks):
            print(f"â¸ï¸ {BATCH_DELAY}ì´ˆ íœ´ì‹...")
            time.sleep(BATCH_DELAY)
    
    print(f"ğŸ‰ {batch_type} ì¢…ëª© ì™„ë£Œ: {total_updated:,}ê°œ ì—…ë°ì´íŠ¸, {total_records:,}ê°œ ë ˆì½”ë“œ")
    print(f"ğŸ“Š ì„±ê³µë¥ : {kis_client.success_count:,}ê°œ ì„±ê³µ, {kis_client.error_count}ê°œ ì‹¤íŒ¨")
    
    return {
        'batch_type': batch_type,
        'updated_count': total_updated,
        'total_records': total_records,
        'error_count': total_errors,
        'api_success_count': kis_client.success_count,
        'api_error_count': kis_client.error_count
    }


def should_update_data(pg_hook: PostgresHook, stock_code: str, trade_date: str) -> bool:
    """í•´ë‹¹ ë‚ ì§œ ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸"""
    
    check_sql = """
    SELECT COUNT(*) 
    FROM domestic_stocks_detail 
    WHERE stock_code = %s 
    AND trade_date = TO_DATE(%s, 'YYYYMMDD')
    AND data_type = 'CHART';
    """
    
    count = pg_hook.get_first(check_sql, parameters=(stock_code, trade_date))[0]
    return count == 0


def save_daily_update_to_db(pg_hook: PostgresHook, stock_code: str, daily_data: Dict, full_response: Dict) -> bool:
    """ì¼ì¼ ì—…ë°ì´íŠ¸ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    
    try:
        # OHLCV ë°ì´í„° ì¶”ì¶œ
        ohlcv_data = {
            'open': int(daily_data['stck_oprc']),
            'high': int(daily_data['stck_hgpr']),
            'low': int(daily_data['stck_lwpr']),
            'close': int(daily_data['stck_clpr']),
            'volume': int(daily_data['acml_vol']),
        }
        
        insert_sql = """
        INSERT INTO domestic_stocks_detail (
            stock_code,
            trade_date,
            current_price,
            volume,
            data_type,
            api_endpoint,
            raw_response,
            request_params,
            request_timestamp,
            created_at,
            updated_at,
            response_code,
            data_quality
        ) VALUES (
            %(stock_code)s,
            TO_DATE(%(trade_date)s, 'YYYYMMDD'),
            %(close_price)s,
            %(volume)s,
            'CHART',
            '/uapi/domestic-stock/v1/quotations/inquire-daily-price',
            %(raw_response)s,
            %(request_params)s,
            NOW(),
            NOW(),
            NOW(),
            '0',
            'EXCELLENT'
        );
        """
        
        with pg_hook.get_conn() as conn:
            with conn.cursor() as cursor:
                cursor.execute(insert_sql, {
                    'stock_code': stock_code,
                    'trade_date': daily_data['stck_bsop_date'],
                    'close_price': ohlcv_data['close'],
                    'volume': ohlcv_data['volume'],
                    'raw_response': json.dumps({
                        **daily_data,
                        'ohlcv': ohlcv_data,
                        'update_source': 'daily_stock_update_dag'
                    }),
                    'request_params': json.dumps({
                        'stock_code': stock_code,
                        'period': '1day',
                        'data_type': 'daily_update',
                        'source': 'scheduled_update'
                    }),
                })
                conn.commit()
        
        return True
        
    except Exception as e:
        print(f"âŒ {stock_code} DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def generate_daily_summary(**context):
    """ì¼ì¼ ì—…ë°ì´íŠ¸ ê²°ê³¼ ìš”ì•½"""
    
    print("ğŸ“Š ì¼ì¼ ì—…ë°ì´íŠ¸ ê²°ê³¼ ìš”ì•½ ìƒì„±...")
    
    # ìš°ì„ ìˆœìœ„ ê²°ê³¼
    priority_result = context['task_instance'].xcom_pull(task_ids='update_priority_stocks') or {}
    regular_result = context['task_instance'].xcom_pull(task_ids='update_regular_stocks') or {}
    
    total_updated = priority_result.get('updated_count', 0) + regular_result.get('updated_count', 0)
    total_records = priority_result.get('total_records', 0) + regular_result.get('total_records', 0)
    total_errors = priority_result.get('error_count', 0) + regular_result.get('error_count', 0)
    
    pg_hook = PostgresHook(postgres_conn_id='quantum_postgres')
    
    # ì˜¤ëŠ˜ ì—…ë°ì´íŠ¸ëœ ë°ì´í„° í™•ì¸
    today_sql = """
    SELECT 
        COUNT(DISTINCT stock_code) as updated_stocks,
        COUNT(*) as updated_records,
        MIN(trade_date) as earliest_date,
        MAX(trade_date) as latest_date
    FROM domestic_stocks_detail 
    WHERE DATE(created_at) = CURRENT_DATE
    AND data_type = 'CHART';
    """
    
    today_stats = pg_hook.get_first(today_sql)
    updated_stocks, updated_records, earliest_date, latest_date = today_stats
    
    # ì „ì²´ ì»¤ë²„ë¦¬ì§€ í™•ì¸
    coverage_sql = """
    SELECT 
        COUNT(*) as total_stocks,
        COUNT(DISTINCT dsd.stock_code) as covered_stocks
    FROM domestic_stocks ds
    LEFT JOIN domestic_stocks_detail dsd ON ds.stock_code = dsd.stock_code
    WHERE ds.is_active = true;
    """
    
    total_stocks, covered_stocks = pg_hook.get_first(coverage_sql)
    coverage_pct = (covered_stocks / total_stocks * 100) if total_stocks > 0 else 0
    
    summary = {
        'execution_date': datetime.now(pytz.timezone("Asia/Seoul")).strftime('%Y-%m-%d'),
        'total_updated_stocks': total_updated,
        'total_records': total_records,
        'total_errors': total_errors,
        'today_updated_stocks': updated_stocks or 0,
        'today_updated_records': updated_records or 0,
        'earliest_date': str(earliest_date) if earliest_date else None,
        'latest_date': str(latest_date) if latest_date else None,
        'total_coverage': {
            'total_stocks': total_stocks,
            'covered_stocks': covered_stocks,
            'coverage_percentage': round(coverage_pct, 2)
        },
        'priority_result': priority_result,
        'regular_result': regular_result,
        'summary_time': datetime.now(pytz.timezone("Asia/Seoul")).isoformat()
    }
    
    print("ğŸ“ˆ ì¼ì¼ ì—…ë°ì´íŠ¸ ìš”ì•½:")
    print(f"  - ì—…ë°ì´íŠ¸ëœ ì¢…ëª©: {total_updated:,}ê°œ")
    print(f"  - ì¶”ê°€ëœ ë ˆì½”ë“œ: {total_records:,}ê°œ")
    print(f"  - ì „ì²´ ì»¤ë²„ë¦¬ì§€: {covered_stocks:,}/{total_stocks:,}ê°œ ({coverage_pct:.1f}%)")
    print(f"  - ì˜¤ëŠ˜ ì‹ ê·œ ë ˆì½”ë“œ: {updated_records or 0:,}ê°œ")
    
    return summary


# ========================================
# DAG íƒœìŠ¤í¬ ì •ì˜
# ========================================

# 1. ê¸°ì¡´ ì¢…ëª© ìƒíƒœ í™•ì¸
get_existing_stocks_task = PythonOperator(
    task_id='get_existing_stocks_status',
    python_callable=get_existing_stocks_status,
    dag=dag,
)

# 2. ìš°ì„ ìˆœìœ„ ì¢…ëª© ì—…ë°ì´íŠ¸
update_priority_stocks_task = PythonOperator(
    task_id='update_priority_stocks',
    python_callable=update_priority_stocks,
    dag=dag,
)

# 3. ì¼ë°˜ ì¢…ëª© ì—…ë°ì´íŠ¸
update_regular_stocks_task = PythonOperator(
    task_id='update_regular_stocks',
    python_callable=update_regular_stocks,
    dag=dag,
)

# 4. ê²°ê³¼ ìš”ì•½
generate_summary_task = PythonOperator(
    task_id='generate_daily_summary',
    python_callable=generate_daily_summary,
    dag=dag,
)

# 5. ì™„ë£Œ ì•Œë¦¼
completion_notification_task = BashOperator(
    task_id='completion_notification',
    bash_command='''
    echo "ğŸ‰ ì¼ì¼ ì¢…ëª© ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!"
    echo "ì‹¤í–‰ ì‹œê°„: $(date '+%Y-%m-%d %H:%M:%S')"
    echo "DAG: Stock_Data__16_Daily_Update" 
    echo "ì£¼ê¸°: ë§¤ì¼ ì˜¤í›„ 9ì‹œ (ì£¼ì¤‘)"
    echo "ëŒ€ìƒ: ì „ì²´ ê¸°ì¡´ ì¢…ëª© ìµœì‹  1ì¼ì¹˜ ë°ì´í„° ì¶”ê°€"
    ''',
    dag=dag,
)

# ========================================
# íƒœìŠ¤í¬ ì˜ì¡´ì„±  
# ========================================
get_existing_stocks_task >> [update_priority_stocks_task, update_regular_stocks_task]
[update_priority_stocks_task, update_regular_stocks_task] >> generate_summary_task >> completion_notification_task